export const benchmarksData = [
  {
    "name": "AIME 2023 I",
    "description": "American Invitational Mathematics Examination 2023 (First)",
    "category": "math",
    "tags": [
      "math",
      "competition",
      "aime",
      "2023"
    ],
    "function_name": "aime_2023_I",
    "is_alpha": false
  },
  {
    "name": "AIME 2023 II",
    "description": "American Invitational Mathematics Examination 2023 (Second)",
    "category": "math",
    "tags": [
      "math",
      "competition",
      "aime",
      "2023"
    ],
    "function_name": "aime_2023_II",
    "is_alpha": false
  },
  {
    "name": "AIME 2024",
    "description": "American Invitational Mathematics Examination 2024 (Combined I & II)",
    "category": "math",
    "tags": [
      "math",
      "competition",
      "aime",
      "2024",
      "combined"
    ],
    "function_name": "aime_2024",
    "is_alpha": false
  },
  {
    "name": "AIME 2024 I",
    "description": "American Invitational Mathematics Examination 2024 (First)",
    "category": "math",
    "tags": [
      "math",
      "competition",
      "aime",
      "2024"
    ],
    "function_name": "aime_2024_I",
    "is_alpha": false
  },
  {
    "name": "AIME 2024 II",
    "description": "American Invitational Mathematics Examination 2024 (Second)",
    "category": "math",
    "tags": [
      "math",
      "competition",
      "aime",
      "2024"
    ],
    "function_name": "aime_2024_II",
    "is_alpha": false
  },
  {
    "name": "AIME 2025",
    "description": "American Invitational Mathematics Examination 2025",
    "category": "math",
    "tags": [
      "math",
      "competition",
      "aime",
      "2025"
    ],
    "function_name": "aime_2025",
    "is_alpha": false
  },
  {
    "name": "AIME 2025 II",
    "description": "American Invitational Mathematics Examination 2025 (Second)",
    "category": "math",
    "tags": [
      "math",
      "competition",
      "aime",
      "2025"
    ],
    "function_name": "aime_2025_II",
    "is_alpha": false
  },
  {
    "name": "ARC-AGI",
    "description": "Abstraction and Reasoning Corpus for Artificial General Intelligence; specify version with -T version=1 or version=2",
    "category": "core",
    "tags": [
      "reasoning",
      "pattern-recognition",
      "abstract-reasoning",
      "visual",
      "logic",
      "agi"
    ],
    "function_name": "arc_agi",
    "is_alpha": false
  },
  {
    "name": "ARC-AGI-1",
    "description": "ARC-AGI dataset version 1",
    "category": "core",
    "tags": [
      "reasoning",
      "pattern-recognition",
      "abstract-reasoning",
      "visual",
      "logic",
      "agi"
    ],
    "function_name": "arc_agi_1",
    "is_alpha": false
  },
  {
    "name": "ARC-AGI-2",
    "description": "ARC-AGI dataset version 2",
    "category": "core",
    "tags": [
      "reasoning",
      "pattern-recognition",
      "abstract-reasoning",
      "visual",
      "logic",
      "agi"
    ],
    "function_name": "arc_agi_2",
    "is_alpha": false
  },
  {
    "is_alpha": false
  },
  {
    "name": "ARC-Challenge",
    "description": "AI2 Reasoning Challenge - Challenging questions from grade-school science exams",
    "category": "core",
    "tags": [
      "multiple-choice",
      "science",
      "commonsense-reasoning"
    ],
    "function_name": "arc_challenge",
    "is_alpha": false
  },
  {
    "name": "ARC-Easy",
    "description": "AI2 Reasoning Challenge - Easy questions from grade-school science exams",
    "category": "core",
    "tags": [
      "multiple-choice",
      "science",
      "commonsense-reasoning"
    ],
    "function_name": "arc_easy",
    "is_alpha": false
  },
  {
    "name": "BBH: Causal Judgment",
    "description": "BigBench Hard - Causal judgment reasoning",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought"
    ],
    "function_name": "bbh_causal_judgment",
    "is_alpha": false
  },
  {
    "name": "BBH: Date Understanding",
    "description": "BigBench Hard - Understanding and reasoning about dates",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought"
    ],
    "function_name": "bbh_date_understanding",
    "is_alpha": false
  },
  {
    "name": "BBH: Disambiguation QA",
    "description": "BigBench Hard - Pronoun disambiguation in questions",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought"
    ],
    "function_name": "bbh_disambiguation_qa",
    "is_alpha": false
  },
  {
    "name": "BBH: Geometric Shapes",
    "description": "BigBench Hard - Reasoning about geometric shapes",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought",
      "geometry"
    ],
    "function_name": "bbh_geometric_shapes",
    "is_alpha": false
  },
  {
    "name": "BBH: Logical Deduction (3 Objects)",
    "description": "BigBench Hard - Logical deduction with three objects",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought",
      "logic"
    ],
    "function_name": "bbh_logical_deduction_three_objects",
    "is_alpha": false
  },
  {
    "name": "BBH: Logical Deduction (5 Objects)",
    "description": "BigBench Hard - Logical deduction with five objects",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought",
      "logic"
    ],
    "function_name": "bbh_logical_deduction_five_objects",
    "is_alpha": false
  },
  {
    "name": "BBH: Logical Deduction (7 Objects)",
    "description": "BigBench Hard - Logical deduction with seven objects",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought",
      "logic"
    ],
    "function_name": "bbh_logical_deduction_seven_objects",
    "is_alpha": false
  },
  {
    "name": "BBH: Movie Recommendation",
    "description": "BigBench Hard - Movie recommendation reasoning",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought"
    ],
    "function_name": "bbh_movie_recommendation",
    "is_alpha": false
  },
  {
    "name": "BBH: Navigate",
    "description": "BigBench Hard - Spatial navigation reasoning",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought",
      "spatial"
    ],
    "function_name": "bbh_navigate",
    "is_alpha": false
  },
  {
    "name": "BBH: Reasoning About Colored Objects",
    "description": "BigBench Hard - Reasoning about colored objects",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought"
    ],
    "function_name": "bbh_reasoning_about_colored_objects",
    "is_alpha": false
  },
  {
    "name": "BBH: Ruin Names",
    "description": "BigBench Hard - Word manipulation and reasoning",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought",
      "wordplay"
    ],
    "function_name": "bbh_ruin_names",
    "is_alpha": false
  },
  {
    "name": "BBH: Salient Translation Error Detection",
    "description": "BigBench Hard - Detecting translation errors",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought",
      "translation"
    ],
    "function_name": "bbh_salient_translation_error_detection",
    "is_alpha": false
  },
  {
    "name": "BBH: Snarks",
    "description": "BigBench Hard - Understanding sarcasm and irony",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought",
      "sarcasm"
    ],
    "function_name": "bbh_snarks",
    "is_alpha": false
  },
  {
    "name": "BBH: Sports Understanding",
    "description": "BigBench Hard - Sports knowledge and reasoning",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought",
      "sports"
    ],
    "function_name": "bbh_sports_understanding",
    "is_alpha": false
  },
  {
    "name": "BBH: Temporal Sequences",
    "description": "BigBench Hard - Understanding temporal sequences",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought",
      "temporal"
    ],
    "function_name": "bbh_temporal_sequences",
    "is_alpha": false
  },
  {
    "name": "BBH: Tracking Shuffled Objects (3 Objects)",
    "description": "BigBench Hard - Tracking three shuffled objects",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought",
      "tracking"
    ],
    "function_name": "bbh_tracking_shuffled_objects_three_objects",
    "is_alpha": false
  },
  {
    "name": "BBH: Tracking Shuffled Objects (5 Objects)",
    "description": "BigBench Hard - Tracking five shuffled objects",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought",
      "tracking"
    ],
    "function_name": "bbh_tracking_shuffled_objects_five_objects",
    "is_alpha": false
  },
  {
    "name": "BBH: Tracking Shuffled Objects (7 Objects)",
    "description": "BigBench Hard - Tracking seven shuffled objects",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "bigbench",
      "chain-of-thought",
      "tracking"
    ],
    "function_name": "bbh_tracking_shuffled_objects_seven_objects",
    "is_alpha": false
  },
  {
    "name": "BRUMO 2025",
    "description": "Bruno Mathematical Olympiad 2025",
    "category": "math",
    "tags": [
      "math",
      "competition",
      "olympiad",
      "2025"
    ],
    "function_name": "brumo_2025",
    "is_alpha": false
  },
  {
    "name": "BoolQ",
    "description": "BoolQ: A Question Answering Dataset for Boolean Reasoning",
    "category": "core",
    "tags": [
      "boolean-reasoning",
      "question-answering"
    ],
    "function_name": "boolq",
    "is_alpha": false
  },
  {
    "name": "BrowseComp",
    "description": "A Simple Yet Challenging Benchmark for Browsing Agents - evaluates model performance on browsing-related tasks",
    "category": "core",
    "tags": [
      "browsing",
      "web",
      "reasoning",
      "graded"
    ],
    "function_name": "browsecomp",
    "is_alpha": false
  },
  {
    "name": "CTI-Bench ATE",
    "description": "Extracting MITRE ATT&CK techniques from malware and threat descriptions",
    "category": "cybersecurity",
    "tags": [
      "extraction",
      "cybersecurity"
    ],
    "function_name": "cti_bench_ate",
    "is_alpha": false
  },
  {
    "name": "CTI-Bench MCQ",
    "description": "Multiple-choice questions evaluating understanding of CTI standards, threats, detection strategies, and best practices using authoritative sources like NIST and MITRE",
    "category": "cybersecurity",
    "tags": [
      "multiple-choice",
      "cybersecurity",
      "knowledge"
    ],
    "function_name": "cti_bench_mcq",
    "is_alpha": false
  },
  {
    "name": "CTI-Bench RCM",
    "description": "Mapping CVE descriptions to CWE categories to evaluate vulnerability classification ability",
    "category": "cybersecurity",
    "tags": [
      "classification",
      "cybersecurity"
    ],
    "function_name": "cti_bench_rcm",
    "is_alpha": false
  },
  {
    "name": "CTI-Bench VSP",
    "description": "Calculating CVSS scores from vulnerability descriptions to assess severity evaluation skills",
    "category": "cybersecurity",
    "tags": [
      "regression",
      "cybersecurity"
    ],
    "function_name": "cti_bench_vsp",
    "is_alpha": false
  },
  {
    "name": "ClockBench",
    "description": "Clock benchmark - time-based reasoning tasks",
    "category": "community",
    "tags": [
      "time",
      "analog",
      "clock",
      "reasoning"
    ],
    "function_name": "clockbench",
    "is_alpha": false
  },
  {
    "name": "DROP",
    "description": "Reading comprehension benchmark requiring discrete reasoning over paragraphs (arithmetic, counting, sorting)",
    "category": "core",
    "tags": [
      "reading-comprehension",
      "reasoning",
      "arithmetic",
      "counting",
      "sorting"
    ],
    "function_name": "drop",
    "is_alpha": false
  },
  {
    "name": "DetailBench",
    "description": "Tests whether LLMs notify users about wrong facts in a text while they are tasked to translate said text",
    "category": "community",
    "tags": [
      "knowledge",
      "graded",
      "instruction-following"
    ],
    "function_name": "detailbench",
    "is_alpha": false
  },
  {
    "name": "Exercism",
    "description": "Multi-language coding benchmark with real-world programming exercises across Python, Go, JavaScript, Java, and Rust",
    "category": "core",
    "tags": [
      "coding",
      "multi-language",
      "execution",
      "docker"
    ],
    "function_name": "exercism",
    "is_alpha": false
  },
  {
    "name": "Exercism (Go)",
    "description": "Go coding tasks from the Exercism benchmark",
    "category": "core",
    "tags": [
      "coding",
      "go",
      "execution",
      "docker"
    ],
    "function_name": "exercism_go",
    "is_alpha": false
  },
  {
    "name": "Exercism (Java)",
    "description": "Java coding tasks from the Exercism benchmark",
    "category": "core",
    "tags": [
      "coding",
      "java",
      "execution",
      "docker"
    ],
    "function_name": "exercism_java",
    "is_alpha": false
  },
  {
    "name": "Exercism (JavaScript)",
    "description": "JavaScript coding tasks from the Exercism benchmark",
    "category": "core",
    "tags": [
      "coding",
      "javascript",
      "execution",
      "docker"
    ],
    "function_name": "exercism_javascript",
    "is_alpha": false
  },
  {
    "name": "Exercism (Python)",
    "description": "Python coding tasks from the Exercism benchmark",
    "category": "core",
    "tags": [
      "coding",
      "python",
      "execution",
      "docker"
    ],
    "function_name": "exercism_python",
    "is_alpha": false
  },
  {
    "name": "Exercism (Rust)",
    "description": "Rust coding tasks from the Exercism benchmark",
    "category": "core",
    "tags": [
      "coding",
      "rust",
      "execution",
      "docker"
    ],
    "function_name": "exercism_rust",
    "is_alpha": false
  },
  {
    "name": "GMCQ",
    "description": "GitHub Multiple Choice Questions",
    "category": "core",
    "tags": [
      "code-understanding"
    ],
    "function_name": "rootly_gmcq",
    "is_alpha": false
  },
  {
    "name": "GPQA Diamond",
    "description": "Graduate-level Google-Proof Q&A in biology, chemistry, and physics",
    "category": "core",
    "tags": [
      "multiple-choice",
      "science",
      "graduate-level"
    ],
    "function_name": "gpqa_diamond",
    "is_alpha": false
  },
  {
    "name": "GraphWalks",
    "description": "Multi-hop reasoning on graphs - both BFS and parent finding tasks",
    "category": "core",
    "tags": [
      "long-context",
      "graphs",
      "reasoning",
      "alpha"
    ],
    "function_name": "graphwalks",
    "is_alpha": true
  },
  {
    "name": "GraphWalks BFS",
    "description": "Multi-hop reasoning on graphs - BFS traversal tasks only",
    "category": "core",
    "tags": [
      "long-context",
      "graphs",
      "reasoning",
      "bfs",
      "alpha"
    ],
    "function_name": "graphwalks_bfs",
    "is_alpha": true
  },
  {
    "name": "GraphWalks Parents",
    "description": "Multi-hop reasoning on graphs - parent finding tasks only",
    "category": "core",
    "tags": [
      "long-context",
      "graphs",
      "reasoning",
      "parents",
      "alpha"
    ],
    "function_name": "graphwalks_parents",
    "is_alpha": true
  },
  {
    "name": "HMMT Feb 2023",
    "description": "Harvard-MIT Mathematics Tournament February 2023",
    "category": "math",
    "tags": [
      "math",
      "competition",
      "hmmt",
      "2023"
    ],
    "function_name": "hmmt_feb_2023",
    "is_alpha": false
  },
  {
    "name": "HMMT Feb 2024",
    "description": "Harvard-MIT Mathematics Tournament February 2024",
    "category": "math",
    "tags": [
      "math",
      "competition",
      "hmmt",
      "2024"
    ],
    "function_name": "hmmt_feb_2024",
    "is_alpha": false
  },
  {
    "name": "HMMT Feb 2025",
    "description": "Harvard-MIT Mathematics Tournament February 2025",
    "category": "math",
    "tags": [
      "math",
      "competition",
      "hmmt",
      "2025"
    ],
    "function_name": "hmmt_feb_2025",
    "is_alpha": false
  },
  {
    "name": "HeadQA",
    "description": "Spanish healthcare specialization exam questions (Spanish and English)",
    "category": "core",
    "tags": [
      "multiple-choice",
      "medical",
      "healthcare",
      "multilingual"
    ],
    "function_name": "headqa",
    "is_alpha": false
  },
  {
    "name": "HeadQA (English)",
    "description": "Spanish healthcare specialization exam questions in English",
    "category": "core",
    "tags": [
      "multiple-choice",
      "medical",
      "healthcare",
      "english"
    ],
    "function_name": "headqa_en",
    "is_alpha": false
  },
  {
    "name": "HeadQA (Spanish)",
    "description": "Spanish healthcare specialization exam questions in Spanish",
    "category": "core",
    "tags": [
      "multiple-choice",
      "medical",
      "healthcare",
      "spanish"
    ],
    "function_name": "headqa_es",
    "is_alpha": false
  },
  {
    "name": "HealthBench",
    "description": "Medical dialogue evaluation using physician-created rubrics for assessing healthcare conversations",
    "category": "core",
    "tags": [
      "medical",
      "dialogue",
      "graded",
      "rubric-based"
    ],
    "function_name": "healthbench",
    "is_alpha": false
  },
  {
    "name": "HealthBench Consensus",
    "description": "Medical dialogue cases with strong physician consensus on appropriate responses",
    "category": "core",
    "tags": [
      "medical",
      "dialogue",
      "graded",
      "rubric-based",
      "consensus"
    ],
    "function_name": "healthbench_consensus",
    "is_alpha": false
  },
  {
    "name": "HealthBench Hard",
    "description": "Most challenging medical dialogue cases from HealthBench requiring nuanced medical knowledge",
    "category": "core",
    "tags": [
      "medical",
      "dialogue",
      "graded",
      "rubric-based",
      "hard"
    ],
    "function_name": "healthbench_hard",
    "is_alpha": false
  },
  {
    "name": "HellaSwag",
    "description": "Adversarially-filtered sentence completion benchmark for commonsense reasoning",
    "category": "core",
    "tags": [
      "multiple-choice",
      "commonsense-reasoning",
      "sentence-completion"
    ],
    "function_name": "hellaswag",
    "is_alpha": false
  },
  {
    "name": "HumanEval",
    "description": "Code generation benchmark with 164 programming problems",
    "category": "core",
    "tags": [
      "coding",
      "generation",
      "execution"
    ],
    "function_name": "humaneval",
    "is_alpha": false
  },
  {
    "name": "Humanity's Last Exam",
    "description": "Multi-modal benchmark at the frontier of human knowledge - 2,500 questions across mathematics, humanities, and natural sciences designed by subject-matter experts globally",
    "category": "core",
    "tags": [
      "knowledge",
      "reasoning",
      "multi-modal",
      "graded",
      "frontier"
    ],
    "function_name": "hle",
    "is_alpha": false
  },
  {
    "name": "Humanity's Last Exam (Text-Only)",
    "description": "Text-only variant of HLE with multi-modal questions filtered out - evaluates models without vision capabilities on text-based questions from the frontier of human knowledge",
    "category": "core",
    "tags": [
      "knowledge",
      "reasoning",
      "text-only",
      "graded",
      "frontier"
    ],
    "function_name": "hle_text",
    "is_alpha": false
  },
  {
    "name": "Instruction Following",
    "description": "Tests ability to follow specific formatting and content constraints with both strict and loose evaluation metrics",
    "category": "core",
    "tags": [
      "instruction-following",
      "constraints",
      "formatting"
    ],
    "function_name": "ifeval",
    "is_alpha": false
  },
  {
    "name": "JSONSchemaBench",
    "description": "JSON Schema generation benchmark with ~10K real-world schemas from GitHub, Kubernetes, and other sources for evaluating constrained decoding",
    "category": "core",
    "tags": [
      "json",
      "jsonschema",
      "generation",
      "constrained-decoding"
    ],
    "function_name": "jsonschemabench",
    "is_alpha": false
  },
  {
    "name": "LiveMCPBench",
    "description": "Benchmark for evaluating LLM agents on real-world tasks using the Model Context Protocol (MCP) - 95 tasks across different categories",
    "category": "core",
    "tags": [
      "mcp",
      "agents",
      "real-world",
      "tools",
      "graded"
    ],
    "function_name": "livemcpbench",
    "is_alpha": false
  },
  {
    "name": "MATH",
    "description": "Measuring Mathematical Problem Solving - 5000 competition math problems across 7 subjects and 5 difficulty levels",
    "category": "core",
    "tags": [
      "math",
      "problem-solving",
      "reasoning",
      "competition",
      "graded"
    ],
    "function_name": "math",
    "is_alpha": false
  },
  {
    "name": "MATH-500",
    "description": "500-problem subset of MATH dataset for faster evaluation of mathematical problem solving",
    "category": "core",
    "tags": [
      "math",
      "problem-solving",
      "reasoning",
      "competition",
      "graded",
      "subset"
    ],
    "function_name": "math_500",
    "is_alpha": false
  },
  {
    "name": "MBPP",
    "description": "Mostly Basic Python Problems â€” code generation tasks with unit test verification",
    "category": "core",
    "tags": [
      "code",
      "generation",
      "sandbox",
      "reasoning"
    ],
    "function_name": "mbpp",
    "is_alpha": false
  },
  {
    "name": "MGSM",
    "description": "Multilingual Grade School Math benchmark across 11 languages for testing mathematical reasoning",
    "category": "core",
    "tags": [
      "math",
      "multilingual",
      "reasoning",
      "chain-of-thought"
    ],
    "function_name": "mgsm",
    "is_alpha": false
  },
  {
    "name": "MGSM English",
    "description": "Grade school math problems in English for testing mathematical reasoning",
    "category": "core",
    "tags": [
      "math",
      "english",
      "reasoning",
      "chain-of-thought"
    ],
    "function_name": "mgsm_en",
    "is_alpha": false
  },
  {
    "name": "MGSM Latin Script",
    "description": "Grade school math problems in Latin script languages (German, English, Spanish, French, Swahili)",
    "category": "core",
    "tags": [
      "math",
      "multilingual",
      "latin-script",
      "reasoning",
      "chain-of-thought"
    ],
    "function_name": "mgsm_latin",
    "is_alpha": false
  },
  {
    "name": "MGSM Non-Latin Script",
    "description": "Grade school math problems in non-Latin script languages (Bengali, Japanese, Russian, Telugu, Thai, Chinese)",
    "category": "core",
    "tags": [
      "math",
      "multilingual",
      "non-latin-script",
      "reasoning",
      "chain-of-thought"
    ],
    "function_name": "mgsm_non_latin",
    "is_alpha": false
  },
  {
    "name": "MMLU (cais/mmlu)",
    "description": "Massive Multitask Language Understanding - 57 academic subjects from the cais/mmlu dataset. Only supports English (EN-US).",
    "category": "core",
    "tags": [
      "multiple-choice",
      "knowledge",
      "reasoning",
      "multitask"
    ],
    "function_name": "mmlu",
    "is_alpha": false
  },
  {
    "name": "MMLU Pro (TIGER-Lab)",
    "description": "Enhanced version of MMLU with more challenging, reasoning-focused questions.",
    "category": "core",
    "tags": [
      "multiple-choice",
      "knowledge",
      "reasoning",
      "multitask"
    ],
    "function_name": "mmlu_pro",
    "is_alpha": false
  },
  {
    "name": "MMMLU (openai/MMMLU)",
    "description": "MMLU translated to 15 languages.",
    "category": "core",
    "tags": [
      "multiple-choice",
      "knowledge",
      "reasoning",
      "multitask"
    ],
    "function_name": "mmmlu",
    "is_alpha": false
  },
  {
    "name": "MMMU",
    "description": "Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark with 11.5K questions across 30 subjects from college exams, quizzes, and textbooks",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "reasoning",
      "college-level",
      "images"
    ],
    "function_name": "mmmu",
    "is_alpha": false
  },
  {
    "name": "MMMU Accounting",
    "description": "MMMU Accounting subset focusing on accounting principles and practices",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "accounting",
      "business",
      "images"
    ],
    "function_name": "mmmu_accounting",
    "is_alpha": false
  },
  {
    "name": "MMMU Agriculture",
    "description": "MMMU Agriculture subset focusing on agricultural sciences and practices",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "agriculture",
      "science",
      "images"
    ],
    "function_name": "mmmu_agriculture",
    "is_alpha": false
  },
  {
    "name": "MMMU Architecture and Engineering",
    "description": "MMMU Architecture and Engineering subset focusing on engineering design and architecture",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "architecture",
      "engineering",
      "design",
      "images"
    ],
    "function_name": "mmmu_architecture_and_engineering",
    "is_alpha": false
  },
  {
    "name": "MMMU Art",
    "description": "MMMU Art subset focusing on art and visual design questions",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "art",
      "visual-design",
      "images"
    ],
    "function_name": "mmmu_art",
    "is_alpha": false
  },
  {
    "name": "MMMU Art Theory",
    "description": "MMMU Art Theory subset focusing on art history and theoretical concepts",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "art",
      "theory",
      "history",
      "images"
    ],
    "function_name": "mmmu_art_theory",
    "is_alpha": false
  },
  {
    "name": "MMMU Basic Medical Science",
    "description": "MMMU Basic Medical Science subset focusing on fundamental medical knowledge",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "medicine",
      "science",
      "health",
      "images"
    ],
    "function_name": "mmmu_basic_medical_science",
    "is_alpha": false
  },
  {
    "name": "MMMU Biology",
    "description": "MMMU Biology subset focusing on biological sciences",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "biology",
      "science",
      "images"
    ],
    "function_name": "mmmu_biology",
    "is_alpha": false
  },
  {
    "name": "MMMU Chemistry",
    "description": "MMMU Chemistry subset focusing on chemical sciences",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "chemistry",
      "science",
      "images"
    ],
    "function_name": "mmmu_chemistry",
    "is_alpha": false
  },
  {
    "name": "MMMU Clinical Medicine",
    "description": "MMMU Clinical Medicine subset focusing on clinical medical practice",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "medicine",
      "clinical",
      "health",
      "images"
    ],
    "function_name": "mmmu_clinical_medicine",
    "is_alpha": false
  },
  {
    "name": "MMMU Design",
    "description": "MMMU Design subset focusing on design principles and practices",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "design",
      "visual",
      "creative",
      "images"
    ],
    "function_name": "mmmu_design",
    "is_alpha": false
  },
  {
    "name": "MMMU Diagnostics and Laboratory Medicine",
    "description": "MMMU Diagnostics and Laboratory Medicine subset focusing on medical diagnostics",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "medicine",
      "diagnostics",
      "laboratory",
      "images"
    ],
    "function_name": "mmmu_diagnostics_and_laboratory_medicine",
    "is_alpha": false
  },
  {
    "name": "MMMU Electronics",
    "description": "MMMU Electronics subset focusing on electronic systems and circuits",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "electronics",
      "engineering",
      "technology",
      "images"
    ],
    "function_name": "mmmu_electronics",
    "is_alpha": false
  },
  {
    "name": "MMMU Energy and Power",
    "description": "MMMU Energy and Power subset focusing on energy systems and power generation",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "energy",
      "power",
      "engineering",
      "images"
    ],
    "function_name": "mmmu_energy_and_power",
    "is_alpha": false
  },
  {
    "name": "MMMU Finance",
    "description": "MMMU Finance subset focusing on financial concepts and analysis",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "finance",
      "business",
      "economics",
      "images"
    ],
    "function_name": "mmmu_finance",
    "is_alpha": false
  },
  {
    "name": "MMMU Geography",
    "description": "MMMU Geography subset focusing on geographical knowledge and analysis",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "geography",
      "earth-science",
      "spatial",
      "images"
    ],
    "function_name": "mmmu_geography",
    "is_alpha": false
  },
  {
    "name": "MMMU History",
    "description": "MMMU History subset focusing on historical knowledge and analysis",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "history",
      "humanities",
      "culture",
      "images"
    ],
    "function_name": "mmmu_history",
    "is_alpha": false
  },
  {
    "name": "MMMU Literature",
    "description": "MMMU Literature subset focusing on literary analysis and knowledge",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "literature",
      "humanities",
      "language",
      "images"
    ],
    "function_name": "mmmu_literature",
    "is_alpha": false
  },
  {
    "name": "MMMU MCQ",
    "description": "MMMU MCQ subset focusing on multiple choice questions",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "images"
    ],
    "function_name": "mmmu_mcq",
    "is_alpha": false
  },
  {
    "name": "MMMU Management",
    "description": "MMMU Management subset focusing on management principles and practices",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "management",
      "business",
      "leadership",
      "images"
    ],
    "function_name": "mmmu_manage",
    "is_alpha": false
  },
  {
    "name": "MMMU Marketing",
    "description": "MMMU Marketing subset focusing on marketing strategies and concepts",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "marketing",
      "business",
      "communication",
      "images"
    ],
    "function_name": "mmmu_marketing",
    "is_alpha": false
  },
  {
    "name": "MMMU Materials",
    "description": "MMMU Materials subset focusing on materials science and engineering",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "materials",
      "science",
      "engineering",
      "images"
    ],
    "function_name": "mmmu_materials",
    "is_alpha": false
  },
  {
    "name": "MMMU Math",
    "description": "MMMU Mathematics subset focusing on mathematical reasoning",
    "category": "math",
    "tags": [
      "multimodal",
      "multiple-choice",
      "mathematics",
      "reasoning",
      "images"
    ],
    "function_name": "mmmu_math",
    "is_alpha": false
  },
  {
    "name": "MMMU Mechanical Engineering",
    "description": "MMMU Mechanical Engineering subset focusing on mechanical systems and design",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "mechanical",
      "engineering",
      "design",
      "images"
    ],
    "function_name": "mmmu_mechanical_engineering",
    "is_alpha": false
  },
  {
    "name": "MMMU Music",
    "description": "MMMU Music subset focusing on music theory and analysis",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "music",
      "arts",
      "theory",
      "images"
    ],
    "function_name": "mmmu_music",
    "is_alpha": false
  },
  {
    "name": "MMMU Open",
    "description": "MMMU Open subset focusing on open-ended questions",
    "category": "core",
    "tags": [
      "multimodal",
      "open-ended",
      "images"
    ],
    "function_name": "mmmu_open",
    "is_alpha": false
  },
  {
    "name": "MMMU Pharmacy",
    "description": "MMMU Pharmacy subset focusing on pharmaceutical sciences and practice",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "pharmacy",
      "medicine",
      "health",
      "images"
    ],
    "function_name": "mmmu_pharmacy",
    "is_alpha": false
  },
  {
    "name": "MMMU Physics",
    "description": "MMMU Physics subset focusing on physics and physical sciences",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "physics",
      "science",
      "images"
    ],
    "function_name": "mmmu_physics",
    "is_alpha": false
  },
  {
    "name": "MMMU Public Health",
    "description": "MMMU Public Health subset focusing on public health concepts and practices",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "public-health",
      "health",
      "population",
      "images"
    ],
    "function_name": "mmmu_public_health",
    "is_alpha": false
  },
  {
    "name": "MMMU Sociology",
    "description": "MMMU Sociology subset focusing on sociological concepts and analysis",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "sociology",
      "social-science",
      "society",
      "images"
    ],
    "function_name": "mmmu_sociology",
    "is_alpha": false
  },
  {
    "name": "MMMU-Pro",
    "description": "Enhanced multimodal MMMU-Pro benchmark with multiple-choice across many options and images",
    "category": "core",
    "tags": [
      "multimodal",
      "multiple-choice",
      "reasoning",
      "images",
      "mmmu-pro"
    ],
    "function_name": "mmmu_pro",
    "is_alpha": false
  },
  {
    "name": "MMMU-Pro (Vision)",
    "description": "MMMU-Pro vision subset with images and multiple-choice questions",
    "category": "core",
    "tags": [
      "multimodal",
      "vision",
      "multiple-choice",
      "images",
      "mmmu-pro"
    ],
    "function_name": "mmmu_pro_vision",
    "is_alpha": false
  },
  {
    "name": "MMStar",
    "description": "MMStar benchmark for measuring multi-modal gain and leakage via coordinated vision and text ablations",
    "category": "core",
    "tags": [
      "vision",
      "multi-modal",
      "leakage",
      "perception",
      "reasoning"
    ],
    "function_name": "mmstar",
    "is_alpha": false
  },
  {
    "name": "MedMCQA",
    "description": "Medical multiple-choice questions from Indian medical entrance exams (AIIMS & NEET PG)",
    "category": "core",
    "tags": [
      "multiple-choice",
      "medical",
      "healthcare",
      "medicine"
    ],
    "function_name": "medmcqa",
    "is_alpha": false
  },
  {
    "name": "MedQA",
    "description": "US Medical Licensing Exam (USMLE) questions for medical reasoning",
    "category": "core",
    "tags": [
      "multiple-choice",
      "medical",
      "healthcare",
      "medicine",
      "clinical"
    ],
    "function_name": "medqa",
    "is_alpha": false
  },
  {
    "name": "MuSR",
    "description": "Testing the Limits of Chain-of-thought with Multistep Soft Reasoning - includes murder mysteries, object placements, and team allocation tasks",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "commonsense",
      "chain-of-thought"
    ],
    "function_name": "musr",
    "is_alpha": false
  },
  {
    "name": "MuSR Murder Mysteries",
    "description": "MuSR murder mystery scenarios - who is the most likely murderer?",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "commonsense",
      "chain-of-thought",
      "murder-mysteries"
    ],
    "function_name": "musr_murder_mysteries",
    "is_alpha": false
  },
  {
    "name": "MuSR Object Placements",
    "description": "MuSR object placement reasoning - where would someone look for an object?",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "commonsense",
      "chain-of-thought",
      "object-placements"
    ],
    "function_name": "musr_object_placements",
    "is_alpha": false
  },
  {
    "name": "MuSR Team Allocation",
    "description": "MuSR team allocation problems - how to allocate people to tasks efficiently?",
    "category": "core",
    "tags": [
      "multiple-choice",
      "reasoning",
      "commonsense",
      "chain-of-thought",
      "team-allocation"
    ],
    "function_name": "musr_team_allocation",
    "is_alpha": false
  },
  {
    "name": "OpenAI MRCR (2 Needles)",
    "description": "Memory-Recall with Contextual Retrieval - long-context evaluation that measures recall of 2 needles across million-token contexts",
    "category": "core",
    "tags": [
      "long-context",
      "retrieval",
      "needle",
      "sequence-matching"
    ],
    "function_name": "openai_mrcr_2n",
    "is_alpha": false
  },
  {
    "name": "OpenAI MRCR (4 Needles)",
    "description": "Memory-Recall with Contextual Retrieval - long-context evaluation that measures recall of 4 needles across million-token contexts",
    "category": "core",
    "tags": [
      "long-context",
      "retrieval",
      "needle",
      "sequence-matching"
    ],
    "function_name": "openai_mrcr_4n",
    "is_alpha": false
  },
  {
    "name": "OpenAI MRCR (8 Needles)",
    "description": "Memory-Recall with Contextual Retrieval - long-context evaluation that measures recall of 8 needles across million-token contexts",
    "category": "core",
    "tags": [
      "long-context",
      "retrieval",
      "needle",
      "sequence-matching"
    ],
    "function_name": "openai_mrcr_8n",
    "is_alpha": false
  },
  {
    "name": "OpenAI MRCR (Full)",
    "description": "Memory-Recall with Contextual Retrieval - long-context evaluation that measures recall of 2, 4, and 8 needles across million-token contexts",
    "category": "core",
    "tags": [
      "long-context",
      "retrieval",
      "needle",
      "sequence-matching"
    ],
    "function_name": "openai_mrcr",
    "is_alpha": false
  },
  {
    "name": "OpenBookQA",
    "description": "Elementary-level science questions probing understanding of core facts",
    "category": "core",
    "tags": [
      "multiple-choice",
      "science",
      "elementary",
      "open-book"
    ],
    "function_name": "openbookqa",
    "is_alpha": false
  },
  {
    "name": "PubMedQA",
    "description": "Biomedical question answering from PubMed abstracts",
    "category": "core",
    "tags": [
      "multiple-choice",
      "medical",
      "biomedical",
      "research",
      "literature"
    ],
    "function_name": "pubmedqa",
    "name": "PIQA",
    "description": "Physical Interaction Question Answering - commonsense about physical situations",
    "category": "core",
    "tags": [
      "multiple-choice",
      "commonsense-reasoning",
      "physical-reasoning"
    ],
    "function_name": "piqa",
    "is_alpha": false
  },
  {
    "name": "PROST",
    "description": "Physical Reasoning about Objects through Space and Time",
    "category": "core",
    "tags": [
      "multiple-choice",
      "commonsense-reasoning",
      "physical-reasoning"
    ],
    "function_name": "prost",
    "is_alpha": false
  },
  {
    "name": "SWAG",
    "description": "Situations With Adversarial Generations - grounded commonsense inference",
    "category": "core",
    "tags": [
      "multiple-choice",
      "commonsense-reasoning",
      "video-captions"
    ],
    "function_name": "swag",
    "is_alpha": false
  },
  {
    "name": "SciCode",
    "description": "Scientific computing and programming challenges",
    "category": "core",
    "tags": [
      "code-generation",
      "science",
      "alpha"
    ],
    "function_name": "scicode",
    "is_alpha": true
  },
  {
    "name": "SimpleQA",
    "description": "Measuring short-form factuality in large language models with simple Q&A pairs",
    "category": "core",
    "tags": [
      "factuality",
      "question-answering",
      "graded"
    ],
    "function_name": "simpleqa",
    "is_alpha": false
  },
  {
    "name": "SuperGPQA",
    "description": "Scaling LLM Evaluation across 285 Graduate Disciplines - 26,529 multiple-choice questions across science, engineering, medicine, economics, and philosophy",
    "category": "core",
    "tags": [
      "multiple-choice",
      "knowledge",
      "graduate-level",
      "multidisciplinary"
    ],
    "function_name": "supergpqa",
    "is_alpha": false
  },
  {
    "name": "TUMLU",
    "description": "TUMLU is a comprehensive, multilingual, and natively developed language understanding benchmark specifically designed for Turkic languages.",
    "category": "community",
    "tags": [
      "factuality",
      "question-answering",
      "multiple-choice",
      "reasoning"
    ],
    "function_name": "tumlu",
    "is_alpha": false
  },
  {
    "name": "Terraform",
    "description": "Terraform Multiple Choice Questions",
    "category": "core",
    "tags": [
      "code-understanding"
    ],
    "function_name": "rootly_terraform",
    "is_alpha": false
  },
  {
    "name": "WSC273",
    "description": "Original Winograd Schema Challenge with 273 expert-crafted questions",
    "category": "core",
    "tags": [
      "multiple-choice",
      "commonsense-reasoning",
      "pronoun-resolution"
    ],
    "function_name": "wsc273",
    "is_alpha": false
  },
  {
    "name": "WinoGrande",
    "description": "Large-scale Winograd Schema Challenge for commonsense pronoun resolution",
    "category": "core",
    "tags": [
      "multiple-choice",
      "commonsense-reasoning",
      "pronoun-resolution"
    ],
    "function_name": "winogrande",
    "is_alpha": false
  }
];
